{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os, sys, gc \n",
    "from plotnine import *\n",
    "import plotnine\n",
    "\n",
    "from tqdm.notebook import tqdm as tqdm_notebook\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.font_manager as fm\n",
    "import matplotlib as mpl\n",
    "from matplotlib import rc\n",
    "import re\n",
    "from matplotlib.ticker import PercentFormatter\n",
    "import datetime\n",
    "from math import log # IDF 계산을 위해"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie = pd.read_csv(\"./ratings.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "user2idx = {}\n",
    "for i, l in enumerate(movie['userId'].unique()):\n",
    "    user2idx[l] = i\n",
    "\n",
    "movie2idx = {}\n",
    "for i, l in enumerate(movie['movieId'].unique()):\n",
    "    movie2idx[l] = i\n",
    "\n",
    "idx2user = {i: user for user, i in user2idx.items()}\n",
    "idx2movie = {i: item for item, i in movie2idx.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "useridx = movie['useridx'] = movie['userId'].apply(lambda x: user2idx[x]).values\n",
    "movieidx = movie['movieidx'] = movie['movieId'].apply(lambda x: movie2idx[x]).values\n",
    "rating = movie['rating'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_users = movie['userId'].nunique()\n",
    "n_items = movie['movieId'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy\n",
    "ratings = scipy.sparse.csr_matrix((rating, (useridx, movieidx)), shape=(len(set(useridx)), len(set(movieidx))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model\n",
    "- Batch (o)\n",
    "- Batch (x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Batch 를 활용한 Matrix Factorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "class MovieLenseDataset(Dataset):\n",
    "    def __init__(self, train, label):\n",
    "        self.x = train\n",
    "        self.y = label\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return torch.tensor(self.x[idx]), torch.tensor(self.y[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "import torch.nn.init as weight_init\n",
    "\n",
    "class MatrixFactorization(nn.Module):\n",
    "    def __init__(self,R, n_users, n_items, n_factors=20):\n",
    "        super().__init__() # 부모 클래스(torch.nn.Module)의 init을 불러옴 \n",
    "        self.user_factors = nn.Embedding(n_users, n_factors)\n",
    "        self.item_factors = nn.Embedding(n_items, n_factors)\n",
    "        self.user_bias = nn.Embedding(n_users, 1)\n",
    "        self.item_bias = nn.Embedding(n_items, 1)\n",
    "        \n",
    "        # weight 초기화 \n",
    "        weight_init.xavier_uniform_(self.user_factors.weight)\n",
    "        weight_init.xavier_uniform_(self.item_factors.weight)\n",
    "        \n",
    "        self.user_bias.weight.data.fill_(0.)\n",
    "        self.item_bias.weight.data.fill_(0.)\n",
    "        \n",
    "        # original Matrix \n",
    "        self.R = R\n",
    "        \n",
    "    def forward(self, user, item):\n",
    "        batch_size = len((model.user_factors(train_batch[:, 0]) * model.item_factors(train_batch[:, 1])).sum(1))\n",
    "        pred = (self.user_factors(user) * self.item_factors(item)).sum(1) + (self.user_bias(user) + self.item_bias(item)).view(batch_size)\n",
    "        return pred\n",
    "    \n",
    "    def complete_matrix(self):\n",
    "        return torch.matmul(self.user_factors.weight, self.item_factors.weight.T) + self.user_bias.weight + self.item_bias.weight.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(MovieLenseDataset(np.array(movie[['useridx', 'movieidx']]), np.array(movie['rating'])), batch_size=32, shuffle=True)\n",
    "model = MatrixFactorization(ratings, n_users, n_items, n_factors=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    1/10 Loss: 0.272459\n",
      "Epoch    2/10 Loss: 0.225241\n",
      "Epoch    3/10 Loss: 0.218957\n",
      "Epoch    4/10 Loss: 0.216221\n",
      "Epoch    5/10 Loss: 0.214616\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-121-909ea3e25e2d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     15\u001b[0m         \u001b[0ml2_reg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0membedding\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0muser_factors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0muser_bias\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m             \u001b[0ml2_reg\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mreg_alpha\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0membedding\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0membedding\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem_factors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem_bias\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\tensor.py\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(i)\u001b[0m\n\u001b[0;32m    388\u001b[0m                           \u001b[1;34m'iterations executed (and might lead to errors or silently give '\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    389\u001b[0m                           'incorrect results).', category=RuntimeWarning)\n\u001b[1;32m--> 390\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0miter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    391\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    392\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__hash__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "dev = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=5e-2, weight_decay=1e-5)  # learning rate\n",
    "loss_func = torch.nn.MSELoss()\n",
    "\n",
    "nb_epochs = 10\n",
    "reg_alpha = torch.tensor(0.05)\n",
    "for epoch in tqdm_notebook(range(0, nb_epochs)):\n",
    "    train_loss = 0\n",
    "    for train_batch, label_batch in train_dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        prediction = model(train_batch[:, 0], train_batch[:, 1])\n",
    "        loss = loss_func(prediction, label_batch.to(dev))\n",
    "        \n",
    "        # Regularization Term\n",
    "        l2_reg = torch.tensor(0).float()\n",
    "        for embedding in [model.user_factors, model.user_bias]:\n",
    "            l2_reg += reg_alpha * sum(torch.pow(embedding(train_batch[:, 0]), 2).sum(1))\n",
    "        \n",
    "        for embedding in [model.item_factors, model.item_bias]:\n",
    "            l2_reg += reg_alpha * sum(torch.pow(embedding(train_batch[:, 1]), 2).sum(1))\n",
    "        loss += l2_reg\n",
    "            \n",
    "        loss.backward()\n",
    "        train_loss += loss.item()\n",
    "        optimizer.step()\n",
    "    print('Epoch {:4d}/{} Loss: {:.6f}'.format(epoch+1, nb_epochs, train_loss/len(train_dataloader.dataset)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recommend "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx2rec = {}\n",
    "for u in useridx.key():\n",
    "    item_rec = np.argsort(-torch.matmul(model.user_factors.weight[user2idx[u]], model.item_factors.weight.T).detach().numpy())[0:200]\n",
    "    # 추천에서 제외해야할 항목\n",
    "    item_rec = [idx2movie[x[0]] for x in item_rec if x not in movie[movie['useridx']==u]['movieidx'].unique()][0:100]\n",
    "    idx2rec[idx2user[u]] = item_rec   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx2rec[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
